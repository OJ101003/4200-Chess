{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 1.,\n",
      "        0., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 1., 0.,\n",
      "        0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1.,\n",
      "        1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1.,\n",
      "        0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0.,\n",
      "        1., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 1., 0.,\n",
      "        0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0.,\n",
      "        0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Assuming MATERIAL_LOOKUP and fen_to_binary_piece_positions are defined\n",
    "MATERIAL_LOOKUP = {\n",
    "    'r': '0001', 'n': '0010', 'b': '0011', 'q': '0100', 'k': '0101', 'p': '0110',\n",
    "    'R': '0111', 'N': '1000', 'B': '1001', 'Q': '1010', 'K': '1011', 'P': '1100',\n",
    "    '.': '0000'  # Using '.' to represent empty squares\n",
    "}\n",
    "\n",
    "def fen_to_binary_piece_positions(fen):\n",
    "    piece_positions = fen.split()[0]\n",
    "    binary_piece_positions = ''\n",
    "    for char in piece_positions:\n",
    "        if char.isdigit():\n",
    "            binary_piece_positions += MATERIAL_LOOKUP['.'] * int(char)\n",
    "        elif char in MATERIAL_LOOKUP:\n",
    "            binary_piece_positions += MATERIAL_LOOKUP[char]\n",
    "        else:  # Ignore slashes\n",
    "            continue\n",
    "    return binary_piece_positions\n",
    "\n",
    "def fen_to_binary(fen):\n",
    "    # Convert piece positions to binary\n",
    "    binary_piece_positions = fen_to_binary_piece_positions(fen)\n",
    "    \n",
    "    # Extract additional game state information from FEN\n",
    "    parts = fen.split()\n",
    "    active_color = '0' if parts[1] == 'w' else '1'\n",
    "    castling = parts[2]\n",
    "    castling_binary = ''.join(['1' if char in castling else '0' for char in 'KQkq'])\n",
    "    \n",
    "    # Encode en passant target square\n",
    "    en_passant = parts[3]\n",
    "    if en_passant == '-':\n",
    "        en_passant_binary = '00000000'\n",
    "    else:\n",
    "        en_passant_binary = '0' * (ord(en_passant[0]) - ord('a')) + '1' + '0' * (7 - (ord(en_passant[0]) - ord('a')))\n",
    "    \n",
    "    # Encode halfmove and fullmove numbers\n",
    "    halfmove_clock = format(int(parts[4]), '08b')\n",
    "    fullmove_number = format(int(parts[5]), '08b')\n",
    "    \n",
    "    # Combine all parts into a single binary string\n",
    "    binary_representation = binary_piece_positions + active_color + castling_binary + en_passant_binary + halfmove_clock + fullmove_number\n",
    "    \n",
    "    # Convert binary string to a tensor\n",
    "    binary_tensor = torch.tensor([int(bit) for bit in binary_representation], dtype=torch.float32)\n",
    "    \n",
    "    return binary_tensor\n",
    "\n",
    "# Example FEN string\n",
    "fen = \"rnbqkbnr/pppppppp/8/8/3P4/8/PPP1PPPP/RNBQKBNR w KQkq - 0 1\"\n",
    "binary_tensor = fen_to_binary(fen)\n",
    "print(binary_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chess\n",
    "b = chess.Board('rn1k1b1r/pp1b1ppp/2p5/3pPq1n/3P1p2/2NB1N2/PPPB2PP/R2Q1K1R b - - 7 15')\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fen_to_binary('rn1k1b1r/pp1b1ppp/2p5/3pPq1n/3P1p2/2NB1N2/PPPB2PP/R2Q1K1R b - - 7 15'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37164639\n",
      "fen rnbqkbnr/ppp2ppp/4p3/3p4/2PP4/8/PP2PPPP/RNBQKBNR w KQkq - 0 3 eval 0.09 binary b'\\x08\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x10\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x81\\x00\\x00\\x00\\x00\\x00\\x00\\x00$\\x00\\x00\\x00\\x00\\x00\\x00\\x00B\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xcf\\x000\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x08\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x10\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x81\\x00\\x00\\x00\\x00\\x00\\x00\\x00$\\x00\\x00\\x00\\x00\\x00\\x00\\x00B\\x00\\x00\\x00\\x00\\x10\\x08\\xe7\\x00\\x00\\x00\\x03\\xff\\x0f'\n",
      "b'CAAAAAAAAAAQAAAAAAAAAIEAAAAAAAAAJAAAAAAAAABCAAAAAAAAAADPADAAAAAAAAAAAAAAAAgAAAAAAAAAEAAAAAAAAACBAAAAAAAAACQAAAAAAAAAQgAAAAAQCOcAAAAD/w8='\n",
      "tensor([0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 1.,\n",
      "        0., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 1., 0.,\n",
      "        0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "        1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0.,\n",
      "        1., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 1., 0.,\n",
      "        0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0.,\n",
      "        0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "from peewee import *\n",
    "import base64\n",
    "\n",
    "db = SqliteDatabase(\"test.db\")\n",
    "\n",
    "class Evaluations(Model):\n",
    "  id = IntegerField()\n",
    "  fen = TextField()\n",
    "  binary = BlobField()\n",
    "  eval = FloatField()\n",
    "  \n",
    "\n",
    "  class Meta:\n",
    "    database = db\n",
    "\n",
    "  def return_custom_binary(self):\n",
    "    return(fen_to_binary(self.fen))\n",
    "\n",
    "  def binary_base64(self):\n",
    "    print(f'fen {self.fen} eval {self.eval} binary {self.binary}')\n",
    "    return base64.b64encode(self.binary)\n",
    "db.connect()\n",
    "LABEL_COUNT = 37164639\n",
    "print(LABEL_COUNT)\n",
    "eval = Evaluations.get(Evaluations.id == 4)\n",
    "print(eval.binary_base64())\n",
    "print(eval.return_custom_binary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, IterableDataset, random_split\n",
    "import pytorch_lightning as pl\n",
    "from random import randrange\n",
    "\n",
    "\n",
    "class EvaluationDataset(IterableDataset):\n",
    "  def __init__(self, count):\n",
    "    self.count = count\n",
    "  def __iter__(self):\n",
    "    return self\n",
    "  def __next__(self):\n",
    "    idx = randrange(self.count)\n",
    "    return self[idx]\n",
    "  def __len__(self):\n",
    "    return self.count\n",
    "  def __getitem__(self, idx):\n",
    "    eval = Evaluations.get(Evaluations.id == idx+1)\n",
    "    bin = np.frombuffer(eval.binary, dtype=np.uint8)\n",
    "    bin = np.unpackbits(bin, axis=0).astype(np.single) \n",
    "    bin = eval.return_custom_binary()\n",
    "    eval.eval = max(eval.eval, -15)\n",
    "    eval.eval = min(eval.eval, 15)\n",
    "    ev = np.array([eval.eval]).astype(np.single) \n",
    "    return {'binary':bin, 'eval':ev}    \n",
    "\n",
    "dataset = EvaluationDataset(count=LABEL_COUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start tensorboard.\n",
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir lightning_logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath = 'lightning_logs/chessml',\n",
    "    filename='epoch={epoch}',  # Save files as epoch number and validation loss\n",
    "    every_n_epochs=1,  # Save every epoch\n",
    "    save_weights_only=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name | Type       | Params\n",
      "------------------------------------\n",
      "0 | seq  | Sequential | 1.5 M \n",
      "------------------------------------\n",
      "1.5 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.5 M     Total params\n",
      "6.157     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 43/72588 [03:52<108:48:09,  0.19it/s, v_num=nt-4]\n",
      "Epoch 0:   0%|          | 15/72588 [01:34<127:08:46,  0.16it/s, v_num=nt-4]\n",
      "Epoch 0: 100%|██████████| 72588/72588 [5:11:28<00:00,  3.88it/s, v_num=nt-4]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 72588/72588 [5:11:28<00:00,  3.88it/s, v_num=nt-4]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from collections import OrderedDict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "class EvaluationModel(pl.LightningModule):\n",
    "  def __init__(self,learning_rate=1e-3,batch_size=1024,layer_count=10):\n",
    "    super().__init__()\n",
    "    self.batch_size = batch_size\n",
    "    self.learning_rate = learning_rate\n",
    "    layers = []\n",
    "    layers.append((\"linear-0\", nn.Linear(285, 808)))\n",
    "    layers.append((\"relu-0\", nn.ReLU()))\n",
    "\n",
    "    for i in range(1, layer_count - 1):\n",
    "        layers.append((f\"linear-{i}\", nn.Linear(808, 808)))\n",
    "        layers.append((f\"relu-{i}\", nn.ReLU()))\n",
    "\n",
    "    layers.append((f\"linear-{layer_count - 1}\", nn.Linear(808, 1)))\n",
    "    self.seq = nn.Sequential(OrderedDict(layers))\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.seq(x)\n",
    "\n",
    "  def training_step(self, batch, batch_idx):\n",
    "    x, y = batch['binary'], batch['eval']\n",
    "    y_hat = self(x)\n",
    "    loss = F.l1_loss(y_hat, y)\n",
    "    self.log(\"train_loss\", loss)\n",
    "    return loss\n",
    "\n",
    "  def configure_optimizers(self):\n",
    "    return torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "\n",
    "  def train_dataloader(self):\n",
    "    dataset = EvaluationDataset(count=LABEL_COUNT)\n",
    "    return DataLoader(dataset, batch_size=self.batch_size, num_workers=0, pin_memory=True)  # Adjust num_workers as per your CPU\n",
    "\n",
    "configs = [\n",
    "           {\"layer_count\": 4, \"batch_size\": 512},\n",
    "          #  {\"layer_count\": 6, \"batch_size\": 1024},\n",
    "           ]\n",
    "for config in configs:\n",
    "    version_name = f'{int(time.time())}-batch_size-{config[\"batch_size\"]}-layer_count-{config[\"layer_count\"]}'\n",
    "    logger = TensorBoardLogger(\"lightning_logs\", name=\"chessml\", version=version_name)\n",
    "    trainer = pl.Trainer(accelerator=\"auto\", precision=\"16-mixed\", max_epochs=1,  logger=logger, callbacks = [checkpoint_callback])\n",
    "    model = EvaluationModel(layer_count=config[\"layer_count\"], batch_size=config[\"batch_size\"])\n",
    "    # Uncomment and use the learning rate finder if needed.\n",
    "    trainer.fit(model)\n",
    "    # Remove break if iterating through multiple configs.\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(EvaluationDataset(1).__getitem__(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import OrderedDict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "configs = [\n",
    "           {\"layer_count\": 4, \"batch_size\": 512},\n",
    "          #  {\"layer_count\": 6, \"batch_size\": 1024},\n",
    "           ]\n",
    "for config in configs:\n",
    "    version_name = f'{int(time.time())}-batch_size-{config[\"batch_size\"]}-layer_count-{config[\"layer_count\"]}'\n",
    "    logger = TensorBoardLogger(\"lightning_logs\", name=\"chessml\", version=version_name)\n",
    "    trainer = pl.Trainer(accelerator=\"auto\", precision=\"16-mixed\", max_epochs=1,  logger=logger, callbacks = [checkpoint_callback])\n",
    "    model = EvaluationModel(layer_count=config[\"layer_count\"], batch_size=config[\"batch_size\"])\n",
    "    # Uncomment and use the learning rate finder if needed.\n",
    "    trainer.fit(model)\n",
    "    # Remove break if iterating through multiple configs.\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = 'lightning_logs/chessml/modeltest.ckpt'\n",
    "\n",
    "\n",
    "# trainer.save_checkpoint(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(checkpoint_path)\n",
    "print(checkpoint.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EvaluationModel(layer_count=config[\"layer_count\"], batch_size=config[\"batch_size\"])\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, SVG\n",
    "from random import randrange\n",
    "\n",
    "SVG_BASE_URL = \"https://us-central1-spearsx.cloudfunctions.net/chesspic-fen-image/\" \n",
    "\n",
    "def svg_url(fen):\n",
    "  fen_board = fen.split()[0]\n",
    "  return SVG_BASE_URL + fen_board\n",
    "\n",
    "def show_index(idx):\n",
    "  eval = Evaluations.select().where(Evaluations.id == idx+1).get()\n",
    "  batch = dataset[idx]\n",
    "  x, y = torch.tensor(batch['binary']), torch.tensor(batch['eval'])\n",
    "  print(batch['binary'])\n",
    "  y_hat = model(x)\n",
    "  loss = F.l1_loss(y_hat, y)\n",
    "  print(f'Idx {idx} Eval {y.data[0]:.2f} Prediction {y_hat.data[0]:.2f} Loss {loss:.2f}')\n",
    "  print(f'FEN {eval.fen}')\n",
    "  display(SVG(url=svg_url(eval.fen)))\n",
    "\n",
    "for i in range(1):\n",
    "  idx = randrange(LABEL_COUNT)\n",
    "  show_index(idx)\n",
    "\n",
    "# need to do better on \"tactics\" like 700756"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
